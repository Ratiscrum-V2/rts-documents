# Rendu d√©ploiement

## I. Docker

Pour faire court, Docker c'est un plateforme permettant de lancer des mini VM nomm√© **container** et chaque container correspond √† un service d'une projet. 
L'int√™ret de Docker, c'est qu'on a pas besoin de se faire chier avec des d√©pendances sur le syst√®me, des conflits de versions, etc etc... Tu as Docker d'install√© sur ton serv et √ßa suffit.

Pour cr√©er notre container , on a besoin de cr√©er une **image** Docker. Une image, c'est en quelques sortes un mod√®le de container, un template.
On peut s'en servir pour cr√©er un container (autrement dit, on lance un container √† partir de l'image) ou pour cr√©er une autre image √† partir de celle ci.

Et pour cr√©er cette image, on va utiliser le fichier Dockerfile pr√©sent √† la racine du projet. Il contient toutes les instructions n√©cessaire √† la cr√©ation de notre image.

### I.1 Dockerfile

On va prendre le Dockerfile de la Webapp comme exemple. On peut d√©j√† voir que le Dockerfile est divis√© en 2 stages : une stage de **build** et une stage d'**execution**.
L'int√™ret de cr√©er l'image en 2 stages, c'est d√©j√† pour s√©parer les environnements de build et d'execution.
Le stage de build va servir √† compiler la webapp.
Le stage d'execution va servir a d√©livrer la webapp compil√© au client.

D√©j√†, regardons le stage de build

```Dockerfile
# build stage
FROM node:16-alpine AS builder

WORKDIR /opt/webapp

# config files copy
COPY package.json /opt/webapp/package.json
COPY package-lock.json /opt/webapp/package-lock.json
COPY tsconfig.json /opt/webapp/tsconfig.json
COPY tsconfig.node.json /opt/webapp/tsconfig.node.json
COPY tailwind.config.cjs /opt/webapp/tailwind.config.cjs
COPY postcss.config.cjs /opt/webapp/postcss.config.cjs
COPY vite.config.ts /opt/webapp/vite.config.ts

# install dependencies
RUN npm install

# source files copy
COPY index.html /opt/webapp/index.html
COPY src/ /opt/webapp/src/

RUN npm run build
```

Tout en haut, on a d√©j√† l'instruction `FROM`. Elle sert √† d√©terminer √† partir de quel image on se base pour cr√©er notre stage.
Ici, on a besoin de node pour build la webapp, on va donc se baser sur une image node. J'ai √©galement lock la version √† la version 16 pour √©viter les mauvaises surprises
et j'ai pris la variante **alpine**, car c'est une image beaucoup plus l√©g√®re que l'image de base.

L'instruction `AS` permet de nommer le stage dans lequel on se trouve. Ici, je pr√©cise que ce stage est le stage appel√© **builder**.   
L'instruction `WORKDIR` comme son nom l'indique d√©finit quel est le r√©pertoire de travail. C'est l'√©quivalent d'un cd mais √† l'int√©rieur de l'image docker.  
L'instruction `COPY` permet de copier un fichier de l'ordinateur vers l'image Docker (ex: COPY package.json /opt/webapp/package.json copie le fichier package.json du PC dans le fichier /opt/webapp/package.json de l'image Docker).  
L'instruction `RUN` permet de lancer une commande √† l'int√©rieur de l'image (ex: RUN npm install va lancer la commande npm install dans l'image Docker).

D√©j√† rien qu'avec √ßa, on arrive √† build la webapp. Une fois que la build passe, on passe au stage suivant, le stage d'execution, qui va d√©finir les conditions de fonctionnement de l'image Docker.

```Dockerfile
# Execute stage
FROM nginx:alpine

# nginx conf file required to handle React router
COPY nginx.conf /etc/nginx/conf.d/default.conf
COPY --from=builder /opt/webapp/dist/ /usr/share/nginx/html/

EXPOSE 80
EXPOSE 443

CMD ["nginx", "-g", "daemon off;"]
```

On part d'une image `nginx:alpine` pour avoir un serveur web qui va d√©livrer la webapp compil√© (variante alpine pour avoir une image plus l√©g√®re üòã). On copie un fichier de conf depuis le repo (Fichier n√©cessaire pour g√©rer les routes du client).
Et apr√®s, on copie les fichiers depuis le stage pr√©c√©dent (le r√©pertoire /opt/webapp/dist/) dans le stage actuel (dans le r√©pertoire /usr/share/nginx/html/).  
Apr√®s, on voit qu'on expose les ports 80 et 443. Ca signifie que ces ports sont disponible depuis l'ext√©rieur de l'image. 

Enfin, y'a l'instruction `CMD`. Elle sert √† indiquer la commande √† executer au lancement de l'image.

### I.2 Build de l'image

Une fois le Dockerfile termin√©, on peut build notre image ! üòÑ

Pour ca, on a la commande
```sh
docker build -t <nom de limage> . # sur Linux, g√©n√©ralement, on rajoute "sudo" devant
```

Le `-t` sert √† d√©terminer un nom de l'image pour le retrouver
L'argument (le .) permet de donner le contexte dans lequel on va build l'image. C'est g√©n√©ralement le dossier dans lequel se trouve le Dockerfile.

Une fois la commande lanc√©e, Docker va t√©l√©charger les images n√©cessaires (ici, les images `node:16-alpine` et `nginx:alpine`) et lancer les instructions du Dockerfile.

Une fois que la build est pass√©, l'image est pr√™te √† √™tre utilis√© !

### I.3 Cr√©ation d'un container

Pour lancer notre webapp, on va donc cr√©er un container Docker √† partir de l'image toute fraiche. 

Ca se fait avec la commande 
```sh
docker run --name <nom du container> -p 3000:80 -it <nom de limage> # sur Linux, g√©n√©ralement, on rajoute "sudo" devant
```

L'option `--name` permet de d√©finir un nom pour notre container  
L'option `-p` permet de bind un port du container sur un port physique du host. Par exemple, ici, le port 3000 du PC va correspondre au port 80 du container.  
L'option `-it` permet de bind la console dans laquelle on lance la commande √† la console du container. Ici, ca nous sert juste √† pouvoir faire `CTRL + C` pour arr√™ter nginx (et donc, le container)

Une fois le container lanc√©, on peut acc√©der √† la webapp sur l'URL **http://localhost:3000/** ! üòÑ

## II. Github Actions

Tout √ßa, c'est bien beau, mais si on est oblig√© de build √† chaque mise en prod, ca va vite √™tre chiant. C'est une belle perte de temps et ca serait quand m√™me mieux d'automatiser tout √ßa.

Donc nous avons mis en place de la **CI/CD** pour build automatiquement l'image √† chaque push sur la branche `main`.

### II.1 Keskec√© la CI/CD ?

La **CI/CD** (Continuous Integration / Continuous Deployment) c'est un syst√®me qui permet d'automatiquement build une application et de la d√©ployer en prod dans la foul√©e.
Pour l'instant, on va juste faire un focus sur la partie build üòé

### II.2 Build de l'image automatique

Du coup, pour tout √ßa, on va utiliser le syst√®me de CI de Github : **Github Actions.**

> Les fichiers pour tout √ßa sont dispo dans `.github/workflows`

Les fichiers yml √† l'int√©rieur sont les pipelines Github. Une **pipeline** en CICD, c'est un ensemble d'√©tapes qui am√®nent au d√©ploiement d'une solution.

D√©j√†, on voit que le fichier est d√©limit√© en plusieurs parties : 

```yml
name: Build webapp
```
Ca c'est juste le nom de la pipeline.

```yml
on:
  push:
    branches: [main]
```
Ca, √ßa correspond au **trigger** de cette pipeline. On peut voir ici qu'on d√©clenche cette pipeline √† chaque push sur la branche `main`.

```yml
env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}
```
Ici, ce sont les variables d'environnement.   
Le `REGISTRY`, ca correspond √† l'endroit o√π on va stocker notre image Docker une fois qu'elle a fini de build (ici c'est ghcr.io, le registry d'image Docker de Github)
L'`IMAGE_NAME`, c'est le nom de l'image qu'on va build. Les items entre ${{ ... }}, ce sont des variables de pipelines (a ne pas confondre avec les variables d'environnement), en l'occurence, le nom du repo

```yml
jobs:
  ..........
```
La, c'est le coeur de la pipeline, c'est ici que sont indiqu√© toutes les choses √† faire.


Ici, je vais juste prendre le job `build`. Ca nous donne donc ceci : 

```yml
jobs:
  build:
    runs-on: ubuntu-latest

    permissions:
      contents: read
      packages: write

    steps:
      - uses: actions/checkout@v2
        with:
          submodules: recursive
      - name: Log in to the Container registry
        uses: docker/login-action@v1
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}
      - name: Extract metadata (tags, labels) for Docker
        id: meta
        uses: docker/metadata-action@v3
        with:
          images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
      - name: Build and push Docker image
        uses: docker/build-push-action@v2
        with:
          context: .
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
  .................
```
Si on d√©cortique √ßa, √ßa nous donne : 
```yml
runs-on: ubuntu-latest
```
Ici, on indique sur quel syst√®me tourne notre pipeline. En g√©n√©ral, on privil√©gie une distrib linux, mais on peut avoir parfois besoin de Windows (pour du build d'appli Unity par exemple) ou de MacOS (pour des build d'appli iOS par exemple).

```yml
permissions:
      contents: read
      packages: write
```

La, on pr√©cise que notre job peut lire le contenu du repo github et √©crire dans les registry de packages github.

```yml
steps:
  ............
```
La c'est toutes les √©tapes du job. Ca fonctionne comme ca:
```yml
- name: Log in to the Container registry
  uses: docker/login-action@v1
  with:
      registry: ${{ env.REGISTRY }}
      username: ${{ github.actor }}
      password: ${{ secrets.GITHUB_TOKEN }}
```

`name` : nom de l'√©tape
`uses` : nom de l'action √† executer
`with` : arguments de l'action

> On peut voir parmi les variables le mot cl√© `secrets`. Ce sont des variables secr√®tes qui sont, pour certaines, g√©n√©r√© par Github (comme c'est le cas ici) ou renseign√© par les d√©veloppeurs. On y met des tokens d'authentifications, des mots de passes, des adresses de serveurs etc...
Sur Ratiscrum, elles sont renseign√© dans l'organisation, on peut les retrouver ici : https://github.com/organizations/Ratiscrum-V2/settings/secrets/actions

En soit, rien de bien compliqu√© tout √ßa, c'est toujours un peu le m√™me principe ! üôÇ Ca c'est un peu les instructions de base, pour plus de d√©tails, faut se r√©f√©rer √† la documentation de l'action qu'on utilise.

En gros, les √©tapes du job de build c'est :
- on r√©cup√®re le contenu du repo 
- on se log au registry pour pouvoir y d√©poser notre image Docker
- on extrait les m√©tadonn√©es
- on build l'image et on l'envoie sur le registry github

Une fois que le workflow est setup, on peut push nos fichiers, et on peut consulter l'√©tat de nos CI ici : https://github.com/Ratiscrum-V2/rts-app/actions

![](img/1.png)
![](img/2.png)

## III. Docker Compose

Revenons √† nos containers Docker. Avoir pleins de containers, c'est bien, les faire fonctionner ensemble, c'est mieux üëÄ

Du coup, pour √ßa, j'ai mis en place un **Docker Compose**.

### III.1 Docker quoi ? 

Un Docker compose, c'est un fichier qui sert √† orchestrer le fonctionnement g√©n√©ral de nos containers Docker entre eux. Il sert √† les lier ensemble, √† les configurer ensemble, etc...
Pour Ratiscrum, j'ai cr√©√© un repo `rts-compose` qui contient le Docker compose et tout ce qu'il faut. 

### III.2 Mise en place

Du coup, notre fichier compose est d√©coup√© ainsi :

```yml
version: "3.3"
```
On indique la version de Docker Compose que l'on utilise

```yml
networks:
  web:
    external: true # traefik network
```
Ici, on indique l'existence d'un network Docker appel√© `web` qui a √©t√© cr√©√© √† l'ext√©rieur du Docker Compose.
C'est une sp√©cificit√© de notre serveur, qui fait tourner plusieurs Docker Compose sur un m√™me VPS, √ßa permet de faire cohabiter tout √ßa + facilement.

```yml
services:
  ............
```

Tout les services pris en charge par le Docker Compose. Un service est constitu√© comme √ßa :
```yml
api:
  image: "ghcr.io/ratiscrum-v2/rts-api:main"
  restart: always
  networks:
    - web
  environment:
    NODE_ENV: production
    API_PORT: 80
    POSTGRES_HOST: postgres
    POSTGRES_DB: ${DB_NAME}
    POSTGRES_USER: ${DB_USER}
    POSTGRES_PASSWORD: ${DB_PASSWORD}
    POSTGRES_PORT: 5432
    JWT_SECRET_KEY: ${JWT_SECRET_KEY}
    S3_ACCESS_KEY_ID: ${MINIO_ROOT_USER}
    S3_SECRET_ACCESS_KEY: ${MINIO_ROOT_PASSWORD}
    S3_ENDPOINT: minio:9000
  labels:
  - "traefik.enable=true"
  - "traefik.http.routers.api.rule=Host(`api.${URL}`)"
  - "traefik.http.routers.api.tls=true"
  - "traefik.http.routers.api.tls.certresolver=httpsresolver"
  - "traefik.http.routers.api.entrypoints=websecure"
  - "traefik.http.services.api.loadbalancer.server.port=80"
  - "traefik.http.middlewares.cors-headers.headers.accessControlAllowOriginListRegex=(.*?)"
  depends_on:
    postgres:
      condition: service_healthy
    minio:
      condition: service_started
```

D√©j√†, la cl√© de l'objet yaml, c'est le nom du service (ici `api`)  
Le champ `image` correspond √† l'image sur laquelle se base le container (ici, on lui indique l'image de l'api ratiscrum)  
Le champ `restart` indique les conditions de red√©marrage de l'api (√† always, ca indique de l'api va toujours red√©marrer, peu importe si elle s'est arr√™t√© √† cause d'un crash, du red√©marrage du serv, etc)  
Le champ `networks` indique les r√©seaux auquel est connect√© le container (ici le r√©seau web)  
Le champ `environment` indique les variables d'environnements  
Le champ `labels` sert ici √† indiquer au reverse proxy les r√®gles de ce container, comme son adresse, son port d'√©coute, l'utilisation du https ou encore la politique de cors. Le reverse proxy c'est Traefik et il tourne dans un autre fichier compose sur le serv  
Le champ `depends_on` indique ici sous quel condition on d√©marre l'api. Ici, on peut voir que l'API d√©marre quand le service `minio` est lanc√© et quand le service `postgres` est *healthy* (comprenez "utilisable", en gros).

Pour d√©finir si un container est *healthy*, on renseigne le champ `healthcheck` :

```yml
..........
  postgres: 
    ............
    healthcheck:
      test: ["CMD", "pg_isready", "--dbname=${DB_NAME}", "--host=localhost", "--username=${DB_USER}"]
      timeout: 10s
      retries: 10
```

En gros, il va faire la commande donn√©e dans `test` 10 fois (le nombre qu'on a indiqu√© dans `retries`) et le test √©choue automatiquement si le code d'erreur de la commande n'est pas 0 ou si la commande n'a pas r√©pondu au bout de 10 secondes (la valeur indiqu√©e dans `timeout`)

### III.3 Lancement du projet

Pour lancer maintenant ce Docker Compose, on va lancer la commande :
```sh
docker compose -f docker-compose.yml --env-file .env up # /!\ "docker-compose" sous linux et "docker compose" sous windows
```

> Le fichier .env doit avoir √©t√© renseign√©.

Le Docker Compose va se charger de tout le reste !

### III.4 D√©ploiement du Docker Compose

Maintenant, on a plus qu'a d√©ployer tout √ßa ! Et, tant qu'√† faire, on va automatiser tout √ßa ! üòÑ

J'ai donc mis en place une Github Actions sur le repo `rts-compose` qui va donc se charger de d√©ployer le fichier docker compose sur le serveur √† chaque changement.

> Le d√©ploiement se fait dans le dossier `/home/ratiscrum/prod` du serveur.

## IV. D√©ploiement des services

Maintenant qu'on a notre CI, on peut reprendre nos pipelines pour qu'apr√®s avoir build, elles d√©ploient automatiquement le changement sur le serveur.  
Pour cela, c'est assez simple : reprenons notre pipeline sur la webapp.

On y ajoute un nouveau job, appel√© `deploy`. 
On garde la m√™me structure, mais on rajoute une propri√©t√© 
```yml
needs: build
```

Cette propri√©t√© indique que l'on ne fait ce job qu'apr√®s que le job `build` est termin√© sans erreur.
Ensuite on ajoute un job `ssh-action` qui va executer des commandes √† distance sur le serveur de prod. 

```yml
steps:
  - name: Deploy API
    uses: appleboy/ssh-action@master
    with:
        host: ${{ secrets.SERVER_ADRESS }}
        username: ${{ secrets.SSH_USER }}
        key: ${{ secrets.SSH_PRIVATE_KEY }}
        port: 22
        script: |
        cd prod
        docker-compose -f docker-compose.yml pull webapp
        docker-compose -f docker-compose.yml stop webapp
        docker-compose -f docker-compose.yml rm -f webapp
        docker-compose -f docker-compose.yml --env-file ./.env up -d webapp
```

Si l'on regarde les commandes √† √©xecuter, 

1. On se met dans le dossier de prod
2. On r√©cup√®re l'image du service webapp
3. On stop le container webapp actuellement en route
4. On le supprime
5. On le relance (√† partir de la nouvelle image qu'on a r√©cup√©rer au pr√©alable)

et voil√† üòÑ Maintenant, √† chaque push sur `main`, une fois la build pass√©, la nouvelle version d'une container va √™tre d√©ploy√© sur le serveur ! 

Maintenant, y'a plus qu'√† coder, tout le reste, l'infra s'occupe de d√©ployer üòé 

![](img/3.png)
![](img/4.png)